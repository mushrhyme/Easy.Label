import os
import streamlit.components.v1 as components
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from hashlib import md5
import streamlit as st
try:
    from streamlit.elements.image import image_to_url
except:
    from streamlit.elements.lib.image_utils import image_to_url
import cv2
from paddleocr import PaddleOCR
import traceback

# ì»´í¬ë„ŒíŠ¸ ì„ ì–¸
IS_RELEASE = False
absolute_path = os.path.dirname(os.path.abspath(__file__))
build_path = os.path.join(absolute_path, "frontend/build")
_component_func = components.declare_component("st-detection", path=build_path)

def get_colormap(label_names, colormap_name='gist_rainbow'):
    """ë¼ë²¨ì— ëŒ€í•œ ì»¬ëŸ¬ë§µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    colormap = {} 
    cmap = plt.get_cmap(colormap_name)
    for idx, l in enumerate(label_names):
        rgb = [int(d) for d in np.array(cmap(float(idx)/len(label_names)))*255][:3]
        colormap[l] = ('#%02x%02x%02x' % tuple(rgb))
    return colormap

def detection(
        client, 
        bucket_name, 
        object_name, 
        bboxes=None, 
        labels=None, 
        height=None, 
        width=None, 
        line_width=5.0, 
        use_space=False, 
        key=None,
    ):
    """ê°ì²´ íƒì§€ ë° ì–´ë…¸í…Œì´ì…˜ ì»´í¬ë„ŒíŠ¸ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
    temp_image_path = client.load_image(bucket_name, object_name)
    if not temp_image_path:
        st.error(f"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {object_name}")
        return None

    image = Image.open(temp_image_path)
    original_image_size = image.size
    image_np = np.array(image)

    if height is None or width is None:
        width = 1200
        height = int(original_image_size[1] * (width / original_image_size[0]))

    image.thumbnail(size=(width, height))
    resized_image_size = image.size
    scale = original_image_size[0]/resized_image_size[0]

    image_url = image_to_url(
        image, image.size[0], True, "RGB", "PNG",
        f"detection-{md5(image.tobytes()).hexdigest()}-{key}"
    )
    if image_url.startswith('/'):
        image_url = image_url[1:]

    color_map = get_colormap(labels, colormap_name='gist_rainbow')

    # bbox_info ìƒì„±
    bbox_info = []
    for i, (bbox, label) in enumerate(zip(bboxes, labels)):
        scaled_bbox = [b/scale for b in bbox]
        bbox_info.append({
            'bbox': scaled_bbox,
            'label_id': i,
            'label': label
        })
    
    # ì»´í¬ë„ŒíŠ¸ í˜¸ì¶œì„ ìœ„í•œ args êµ¬ì„±
    component_args = {
        "image_url": image_url,
        "image_size": image.size,
        "bbox_info": bbox_info,
        "color_map": color_map,
        "line_width": line_width,
        "use_space": use_space,
        "ocr_suggestions": st.session_state.get("ocr_result", []),  # OCR ê²°ê³¼ ì „ë‹¬
        "request_ocr": False
    }

    # ì»´í¬ë„ŒíŠ¸ í˜¸ì¶œ
    component_value = _component_func(**component_args, key=key)

    print(f"DEBUG: component_value={component_value}")

    # ë°˜í™˜ê°’ ì—†ìœ¼ë©´ ì•„ì§ ë Œë” ì¤‘
    if component_value is None:
        return None

    if isinstance(component_value, dict) and "bboxes" in component_value:
        request_ocr = component_value.get("request_ocr", False)
        selected_box_id = component_value.get("selected_box_id")
        bbox_data = component_value.get("bboxes", [])
        print(f"DEBUG: request_ocr={request_ocr}, selected_box_id={selected_box_id}, bbox_data={bbox_data}")

        if request_ocr and selected_box_id:
            if st.session_state.get("pending_ocr_request", False):
                print("DEBUG: OCR ìš”ì²­ì´ ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤.")
                return None

            st.session_state.pending_ocr_request = True

            # OCR ê°ì²´ê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
            if st.session_state.get("ocr") is None:
                st.session_state.ocr = PaddleOCR(
                    use_angle_cls=True,
                    show_log=False,
                    lang='korean',
                    det_model_dir='/Users/nongshim/Desktop/Python/project/streamlit_image_annotation/Detection/inference/det_v6',
                    rec_model_dir='/Users/nongshim/Desktop/Python/project/streamlit_image_annotation/Detection/inference/rec_v2_19_best'
                )

            try:
                print(f"DEBUG: OCR ìš”ì²­ë¨: {selected_box_id}")
                selected_idx = int(selected_box_id.split('-')[-1])
                if selected_idx < len(bbox_data):
                    selected_bbox = bbox_data[selected_idx]['bbox']
                    ocr_result = process_ocr_for_bbox_array(image_np, selected_bbox, st.session_state.ocr)

                    if not ocr_result:
                        ocr_result = ["í…ìŠ¤íŠ¸ ì—†ìŒ"]

                    print(f"DEBUG: OCR ê²°ê³¼: {ocr_result}")
                    st.session_state.ocr_result = ocr_result  # âœ… ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    component_value["request_ocr"] = False
                    print(f"request_ocr={component_value['request_ocr']}")
                    st.session_state.request_ocr = False  # âœ… ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    st.session_state.pending_ocr_request = False  # âœ… ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                else:
                    print(f"WARNING: ì„ íƒëœ ë°•ìŠ¤ ì¸ë±ìŠ¤ê°€ ë²”ìœ„ ì´ˆê³¼: {selected_idx}")
            except Exception as e:
                print(f"ERROR: OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                traceback.print_exc()
            finally:
                st.session_state.pending_ocr_request = False

            # # rerun ìœ ë„ â†’ ë‹¤ìŒ ë Œë”ì—ì„œ OCR ê²°ê³¼ê°€ ë°˜ì˜ë¨
            # print("ğŸ” rerun í˜¸ì¶œ ì§ì „")
            # st.rerun()
        return component_value
    
    ocr_result = st.session_state.get("ocr_result", [])

    if "ocr_result" in st.session_state:
        del st.session_state.ocr_result
    if "request_ocr" in st.session_state:
        del st.session_state["request_ocr"]
    
    return {
        "mode": component_value.get("mode", "Draw"),
        "bboxes": component_value.get("bboxes", []),
        "save_requested": component_value.get("save_requested", False),
        "ocr_suggestions": ocr_result,
    }


def detect_tt_regions(image_path, ocr):
    """
    ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì˜ì—­(BBox)ë§Œ ê²€ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    img_np = cv2.imread(image_path)
    if img_np is None:
        img = Image.open(image_path)
        img_np = np.array(img)
    else:
        img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)
    
    # Detection ìˆ˜í–‰
    boxes_result = ocr.text_detector(img_np)
    boxes = boxes_result[0] if boxes_result else []
    
    return img_np, boxes  # ì›ë³¸ ì´ë¯¸ì§€, ê²€ì¶œëœ BBox ë°˜í™˜


def auto_detect_text_regions(bucket_name, object_name, bboxes, labels, image_path):
    """
    MinIOì—ì„œ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì™€ í…ìŠ¤íŠ¸ ì˜ì—­ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ëŠ” í•¨ìˆ˜
    """    
    # ì„ì‹œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    temp_image_path = None
    try:
        # MinIOì—ì„œ ì„ì‹œ íŒŒì¼ë¡œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        temp_image_path = st.session_state.minio_client.load_image(
            bucket_name,
            object_name
        )
        
        if temp_image_path:
            # í…ìŠ¤íŠ¸ ì˜ì—­ ê°ì§€
            _, detected_boxes = detect_text_regions(temp_image_path, st.session_state.ocr)
            # ê°ì§€ëœ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ê¸°ì¡´ ì–´ë…¸í…Œì´ì…˜ì— ì¶”ê°€
            for box in detected_boxes:
                # PaddleOCRì˜ boxëŠ” 4ê°œì˜ ì (ì  4ê°œê°€ x, y ì¢Œí‘œë¥¼ ê°€ì§)ìœ¼ë¡œ êµ¬ì„±ë¨
                # ìµœì†Œ/ìµœëŒ€ x, y ê°’ì„ ì°¾ì•„ì„œ bounding boxë¥¼ ë§Œë“¦
                points = np.array(box)
                min_x, min_y = np.min(points, axis=0)
                max_x, max_y = np.max(points, axis=0)

                # ìƒˆë¡œìš´ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ìƒì„± (x1, y1, x2, y2 í˜•ì‹)
                new_bbox = [float(min_x), float(min_y), float(max_x - min_x), float(max_y - min_y)]
                print("DEBUG: new_bbox", new_bbox)
                # ìƒˆ ë°”ìš´ë”© ë°•ìŠ¤ê°€ ê¸°ì¡´ì— ì—†ìœ¼ë©´ ì¶”ê°€
                if new_bbox not in bboxes:
                    bboxes.append(new_bbox)
                    labels.append("")  # ìë™ ê°ì§€ëœ í…ìŠ¤íŠ¸ì˜ ê¸°ë³¸ ë¼ë²¨
                    
                    # ì„¸ì…˜ ìƒíƒœì˜ ì–´ë…¸í…Œì´ì…˜ ë°ì´í„° ì—…ë°ì´íŠ¸ (ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ DBì— ì €ì¥ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ)
                    if 'annotations' in st.session_state:
                        # í˜„ì¬ ìµœëŒ€ ID ì°¾ê¸°
                        max_id = 0
                        if st.session_state.annotations:
                            for annotation in st.session_state.annotations:
                                if annotation.get("id", 0) > max_id:
                                    max_id = annotation["id"]
                        
                        # ìƒˆ ì–´ë…¸í…Œì´ì…˜ ê°ì²´ ìƒì„±
                        new_annotation = {
                            "id": max_id + 1,
                            "label": "",
                            "bbox": {
                                "x": new_bbox[0],
                                "y": new_bbox[1],
                                "width": new_bbox[2],
                                "height": new_bbox[3]
                            }
                        }
                        print("DEBUG: new_annotation", new_annotation)
                        st.session_state.annotations.append(new_annotation)
  
    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ê°ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if temp_image_path and os.path.exists(temp_image_path):
            os.unlink(temp_image_path)

def preprocess_roi(cropped, target_width=320, target_height=48):
    h, w = cropped.shape[:2]
    ratio = w / float(h)
    
    # ì¢…íš¡ë¹„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë†’ì´ ë§ì¶”ê¸°
    if int(target_height * ratio) <= target_width:
        resized_w = int(target_height * ratio)
        resized = cv2.resize(cropped, (resized_w, target_height))
        
        # íŒ¨ë”© ì¶”ê°€
        processed = np.zeros((target_height, target_width, 3), dtype=np.uint8)
        processed[:, 0:resized_w, :] = resized
    else:
        # ë„ˆë¹„ê°€ ë„ˆë¬´ ê¸¸ë©´ ë„ˆë¹„ì— ë§ì¶”ê³  ë†’ì´ ì¡°ì •
        resized_h = int(target_width / ratio)
        resized = cv2.resize(cropped, (target_width, resized_h))
        
        # íŒ¨ë”© ì¶”ê°€ (ì„¸ë¡œ ì¤‘ì•™ ì •ë ¬)
        processed = np.zeros((target_height, target_width, 3), dtype=np.uint8)
        pad_top = (target_height - resized_h) // 2
        processed[pad_top:pad_top+resized_h, :, :] = resized
    
    return processed

def recognize_text_from_rois(img_np, boxes, ocr, batch_size=32, width=320, height=48):
    """
    ê²€ì¶œëœ BBoxë¥¼ ê¸°ë°˜ìœ¼ë¡œ ROIë¥¼ ì¶”ì¶œí•œ í›„, í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹í•˜ëŠ” í•¨ìˆ˜
    """
    roi_images = []
    valid_boxes = []

    # ROI ì¶”ì¶œ ë° ì „ì²˜ë¦¬
    for points in boxes:
        x_min, x_max = int(min(points[:, 0])), int(max(points[:, 0]))
        y_min, y_max = int(min(points[:, 1])), int(max(points[:, 1]))
        
        cropped = img_np[y_min:y_max, x_min:x_max]
        processed = preprocess_roi(cropped, width, height)  # ì „ì²˜ë¦¬ í•¨ìˆ˜ ì ìš©
        
        roi_images.append(processed)
        valid_boxes.append(points)

    paddle_boxes = []
    paddle_txts = []
    paddle_scores = []

    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ Recognition ìˆ˜í–‰
    for i in range(0, len(roi_images), batch_size):
        batch_images = roi_images[i:i+batch_size]
        batch_boxes = valid_boxes[i:i+batch_size]
        
        rec_results = ocr.text_recognizer(batch_images)
        
        if rec_results is None or len(rec_results[0]) == 0:
            continue
        
        texts = rec_results[0]
        
        for j, (text_info, box) in enumerate(zip(texts, batch_boxes)):
            if text_info is not None:
                text, conf = text_info
                paddle_boxes.append(box.tolist())
                paddle_txts.append(text)
                paddle_scores.append(conf)

    return paddle_boxes, paddle_txts, paddle_scores



# def process_ocr_for_bbox(image_path, bbox, ocr):
#     """
#     ì„ íƒëœ ë°”ìš´ë”© ë°•ìŠ¤ì— ëŒ€í•´ OCR ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
#     Args:
#         image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
#         bbox: ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ [x, y, width, height]
#         ocr: OCR ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    
#     Returns:
#         ì¸ì‹ëœ í…ìŠ¤íŠ¸ ëª©ë¡
#     """
#     default_text = ["ì¶”ì²œ"]  # ê¸°ë³¸ ì¶”ì²œ í…ìŠ¤íŠ¸
#     try:            
#         # ì´ë¯¸ì§€ ë¡œë“œ
#         image = Image.open(image_path)
#         image_np = np.array(image)
        
#         # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ê³„ì‚° (x, y, width, height í˜•ì‹ì—ì„œ í¬ì¸íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
#         x, y, width, height = bbox
#         x1, y1 = int(x), int(y)
#         x2, y2 = int(x + width), int(y + height)
        
#         # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ ë° ì¢Œí‘œ ë³´ì •
#         h, w = image_np.shape[:2]
#         x1, y1 = max(0, x1), max(0, y1)
#         x2, y2 = min(w, x2), min(h, y2)
        
#         # ë²”ìœ„ê°€ ìœ íš¨í•œì§€ í™•ì¸
#         if x1 >= x2 or y1 >= y2 or x1 >= w or y1 >= h:
#             print("DEBUG: ìœ íš¨í•˜ì§€ ì•Šì€ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ")
#             return default_text  # ìœ íš¨í•˜ì§€ ì•Šì€ ì˜ì—­
        
#         # PaddleOCRì—ì„œ ì‚¬ìš©í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ë³€í™˜
#         # ì¢Œìƒ, ìš°ìƒ, ìš°í•˜, ì¢Œí•˜ ìˆœì„œì˜ 4ê°œ ì  ì¢Œí‘œ
#         paddle_box = np.array([
#             [x1, y1],  # ì¢Œìƒë‹¨
#             [x2, y1],  # ìš°ìƒë‹¨
#             [x2, y2],  # ìš°í•˜ë‹¨
#             [x1, y2]   # ì¢Œí•˜ë‹¨
#         ]).reshape(1, 4, 2)  # (1, 4, 2) í˜•íƒœë¡œ ë³€í™˜
        
#         # recognize_text_from_rois í•¨ìˆ˜ í˜¸ì¶œ
#         _, paddle_txts, paddle_scores = recognize_text_from_rois(
#             image_np, paddle_box, ocr, batch_size=1
#         )
        
#         # ê²°ê³¼ í™•ì¸ ë° ë°˜í™˜
#         if paddle_txts and len(paddle_txts) > 0:
#             print(f"DEBUG: ì¸ì‹ëœ í…ìŠ¤íŠ¸: {paddle_txts}, ì ìˆ˜: {paddle_scores}")
            
#             # # ì‹ ë¢°ë„ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§ (ì˜µì…˜)
#             # confidence_threshold = 0.5
#             # filtered_texts = [txt for txt, score in zip(paddle_txts, paddle_scores) if score > confidence_threshold]
            
            
#             return default_text
#         else:
#             print("DEBUG: í…ìŠ¤íŠ¸ê°€ ì¸ì‹ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
#             return default_text  # ì¸ì‹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°
        
#     except Exception as e:
#         print(f"OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         import traceback
#         traceback.print_exc()  # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ ì¶œë ¥
#         return default_text  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
    

def process_ocr_for_bbox_array(image_np, bbox, ocr):
    """
    NumPy ë°°ì—´ í˜•íƒœì˜ ì´ë¯¸ì§€ì—ì„œ ì„ íƒëœ ë°”ìš´ë”© ë°•ìŠ¤ì— ëŒ€í•´ OCR ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        image_np: ì´ë¯¸ì§€ NumPy ë°°ì—´
        bbox: ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ [x, y, width, height]
        ocr: OCR ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    
    Returns:
        ì¸ì‹ëœ í…ìŠ¤íŠ¸ ëª©ë¡
    """
    default_text = ["ì¶”ì²œ"]  # ê¸°ë³¸ ì¶”ì²œ í…ìŠ¤íŠ¸
    try:        
        # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ê³„ì‚°
        x, y, width, height = bbox
        x1, y1 = int(x), int(y)
        x2, y2 = int(x + width), int(y + height)
        
        # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ ë° ì¢Œí‘œ ë³´ì •
        h, w = image_np.shape[:2]
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(w, x2), min(h, y2)
        
        # ë²”ìœ„ê°€ ìœ íš¨í•œì§€ í™•ì¸
        if x1 >= x2 or y1 >= y2 or x1 >= w or y1 >= h:
            print(f"DEBUG: ìœ íš¨í•˜ì§€ ì•Šì€ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ: x1={x1}, y1={y1}, x2={x2}, y2={y2}, w={w}, h={h}")
            return default_text  # ìœ íš¨í•˜ì§€ ì•Šì€ ì˜ì—­
        
        # ì´ë¯¸ì§€ í¬ë¡­
        cropped_image = image_np[y1:y2, x1:x2]
        
        # í¬ë¡­ëœ ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ (ë””ë²„ê¹…ìš©)
        print(f"DEBUG: í¬ë¡­ëœ ì´ë¯¸ì§€ í¬ê¸°: {cropped_image.shape}")
        
        # PaddleOCRì—ì„œ ì‚¬ìš©í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ë³€í™˜
        # ì¢Œìƒ, ìš°ìƒ, ìš°í•˜, ì¢Œí•˜ ìˆœì„œì˜ 4ê°œ ì  ì¢Œí‘œ
        paddle_box = np.array([
            [x1, y1],  # ì¢Œìƒë‹¨
            [x2, y1],  # ìš°ìƒë‹¨
            [x2, y2],  # ìš°í•˜ë‹¨
            [x1, y2]   # ì¢Œí•˜ë‹¨
        ]).reshape(1, 4, 2)  # (1, 4, 2) í˜•íƒœë¡œ ë³€í™˜
        
        # recognize_text_from_rois í•¨ìˆ˜ í˜¸ì¶œ
        paddle_boxes, paddle_txts, paddle_scores = recognize_text_from_rois(
            image_np, paddle_box, ocr, batch_size=1
        )
        
        # ê²°ê³¼ í™•ì¸ ë° ë°˜í™˜
        if paddle_txts and len(paddle_txts) > 0:
            print(f"DEBUG: ì¸ì‹ëœ í…ìŠ¤íŠ¸: {paddle_txts}, ì ìˆ˜: {paddle_scores}")
            
            # # ì‹ ë¢°ë„ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§ (ì˜µì…˜)
            # confidence_threshold = 0.5
            # filtered_texts = [txt for txt, score in zip(paddle_txts, paddle_scores) if score > confidence_threshold]
            
            # "ì¶”ì²œ" í•­ëª©ì„ ë§¨ ì•ì— ì¶”ê°€í•˜ê³  ê²°ê³¼ ë°˜í™˜
            return paddle_txts
        else:
            print("DEBUG: í…ìŠ¤íŠ¸ê°€ ì¸ì‹ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return default_text # ì¸ì‹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°
        
    except Exception as e:
        print(f"OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()  # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ ì¶œë ¥
        return default_text  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜